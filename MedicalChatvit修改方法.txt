1.lora微调blip2 https://zhuanlan.zhihu.com/p/670048482
2.查看vit增加效果策略 
	关于ViT，你必须要知道的三点改进 https://zhuanlan.zhihu.com/p/488574791
	pytorch对网络层的增，删， 改, 修改预训练模型结构 https://blog.csdn.net/qq_53345829/article/details/124641236
	ViT的极简pytorch实现及其即插即用 https://blog.csdn.net/qq_36563273/article/details/135283077
	尝试改变vit里的里的patch_embedding
	
1.modeling_blip_2.py中修改Blip2VisionEmbeddings层
2.finetune_param.py中设置hmlp_stem层可微调
####### yjm start #######
class hMLP_stem(nn.Module):
    """ Image to Patch Embedding
    """
    def __init__(self, img_size=(224,224), patch_size=(16,16), in_chans=3, embed_dim=768):
        super().__init__()
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        self.proj = torch.nn.Sequential(
            *[nn.Conv2d(in_chans, embed_dim//4, kernel_size=4, stride=4),
            nn.SyncBatchNorm(embed_dim//4, eps=1e-06), # 这里采用BN，也可以采用LN
            nn.GELU(),
            nn.Conv2d(embed_dim//4, embed_dim//4, kernel_size=2, stride=2),
            nn.SyncBatchNorm(embed_dim//4, eps=1e-06),
            nn.GELU(),
            nn.Conv2d(embed_dim//4, embed_dim, kernel_size=2, stride=2),
            nn.SyncBatchNorm(embed_dim, eps=1e-06),
        ])
        # 初始化权重
        for m in self.proj.modules():
            if isinstance(m, nn.Conv2d):
                init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
        
    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x)
        return x

class Blip2VisionEmbeddings(nn.Module):
    def __init__(self, config: Blip2VisionConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = config.image_size
        self.patch_size = config.patch_size

        self.class_embedding = nn.Parameter(torch.randn(1, 1, self.embed_dim))

        # 使用 hMLP_stem 代替 patch_embedding
        self.hmlp_stem = hMLP_stem(
            img_size=(self.image_size, self.image_size),
            patch_size=(self.patch_size, self.patch_size),
            in_chans=3,
            embed_dim=self.embed_dim
        )

        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches + 1

        self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))

    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:
        batch_size = pixel_values.shape[0]
        target_dtype = self.position_embedding.dtype

        # 获取图像的嵌入表示
        patch_embeds = self.hmlp_stem(pixel_values.to(target_dtype))
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
        
        class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)
        
        # print("patch_embeds",patch_embeds.shape)
        # print("class_embeds",class_embeds.shape)
        
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
        embeddings = embeddings + self.position_embedding[:, : embeddings.size(1), :].to(target_dtype)
        return embeddings
####### yjm end #######

